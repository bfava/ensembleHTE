% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/analysis_hte.R
\name{blp}
\alias{blp}
\title{Compute BLP (Best Linear Predictor) of CATE}
\usage{
blp(
  ensemble_fit,
  outcome = NULL,
  treatment = NULL,
  prop_score = NULL,
  controls = NULL,
  subset = NULL
)
}
\arguments{
\item{ensemble_fit}{An object of class \code{ensemble_hte_fit} from \code{ensemble_hte()}
or \code{ensemble_pred_fit} from \code{ensemble_pred()}.}

\item{outcome}{Either:
\itemize{
  \item NULL (default): uses the same outcome as in the ensemble function
  \item Character string: column name in the \code{data} used in the ensemble function
  \item Numeric vector: custom outcome variable (must have same length as data)
}
This allows computing BLP for a different outcome than the one used for prediction.}

\item{treatment}{For \code{ensemble_pred_fit} only. The treatment variable:
\itemize{
  \item Character string: column name in the \code{data} used in \code{ensemble_pred()}
  \item Numeric vector: binary treatment variable (must have same length as data)
}
Ignored for \code{ensemble_hte_fit} (uses the treatment from the fit).}

\item{prop_score}{For \code{ensemble_pred_fit} only. Propensity score:
\itemize{
  \item NULL (default): estimated as mean of treatment variable
  \item Numeric value: constant propensity score for all observations
  \item Numeric vector: observation-specific propensity scores
}
For \code{ensemble_hte_fit}, uses the propensity score from the fit.}

\item{controls}{Character vector of control variable names to include in regression.
These must be column names present in the \code{data} argument used when calling
the ensemble function.}

\item{subset}{Which observations to use for the BLP analysis. Options:
\itemize{
  \item NULL (default): uses all observations (or training obs for \code{ensemble_pred_fit}
    when using the default outcome with subset training)
  \item \code{"train"}: uses only training observations (for \code{ensemble_pred_fit} only)
  \item \code{"all"}: explicitly uses all observations
  \item Logical vector: TRUE/FALSE for each observation (must have same length as data)
  \item Integer vector: indices of observations to include (1-indexed)
}
This allows evaluating BLP on a subset of observations. Note that BLP 
requires observations from both treatment and control groups in the subset.}
}
\value{
An object of class `blp_results` containing:
\itemize{
  \item estimates: data.table with BLP estimates averaged across repetitions
  \item outcome: outcome variable used
  \item targeted_outcome: original outcome from ensemble fitting
  \item fit_type: "hte" or "pred" depending on input
  \item controls: control variables used (if any)
  \item M: number of repetitions
  \item call: the function call
}
}
\description{
Computes the Best Linear Predictor (BLP) of the Conditional Average Treatment
Effect (CATE), an estimand introduced by Chernozhukov et al. (2025) to test
whether machine learning predictions capture meaningful treatment effect heterogeneity.

This function works with both \code{ensemble_hte_fit} (uses predicted ITE)
and \code{ensemble_pred_fit} (uses predicted Y). For prediction fits, a
treatment variable must be specified.

This function implements the multiple-split estimation strategy developed by
Fava (2025), which combines predictions from multiple machine learning algorithms
into an ensemble and averages BLP estimates across M repetitions of K-fold
cross-validation to improve statistical power.

The method uses weighted least squares regression of Y on:
\itemize{
  \item W1 = (D - propensity_score): coefficient (beta1) estimates the ATE
  \item W2 = (D - propensity_score) * (predicted - mean(predicted)): coefficient (beta2) tests heterogeneity
}
Weights are set to 1 / (propensity_score * (1 - propensity_score)).
Robust (HC1) standard errors are computed.

A significant beta2 indicates that the ML predictions capture meaningful
treatment effect heterogeneity.
}
\examples{
\donttest{
set.seed(123)
n <- 500
X1 <- rnorm(n); X2 <- rnorm(n)
D <- rbinom(n, 1, 0.5)
Y <- X1 + D * (1 + 0.5 * X1) + rnorm(n)
data <- data.frame(Y = Y, D = D, X1 = X1, X2 = X2)

fit <- ensemble_hte(Y ~ X1 + X2, treatment = D, data = data,
                    algorithms = c("lm", "grf"), M = 3, K = 3)
result <- blp(fit)
print(result)
}

}
\references{
Chernozhukov, V., Demirer, M., Duflo, E., & Fernández-Val, I. (2025).
Fisher–Schultz Lecture: Generic Machine Learning Inference on Heterogeneous
Treatment Effects in Randomized Experiments, with an Application to Immunization
in India. \emph{Econometrica}, 93(4), 1121-1164.

Fava, B. (2025). Training and Testing with Multiple Splits: A Central Limit
Theorem for Split-Sample Estimators. \emph{arXiv preprint arXiv:2511.04957}.
}
