% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/analysis_pred.R
\name{blp_pred}
\alias{blp_pred}
\title{Compute BLP (Best Linear Predictor) for Prediction}
\usage{
blp_pred(ensemble_fit, outcome = NULL, subset = NULL)
}
\arguments{
\item{ensemble_fit}{An object of class `ensemble_pred_fit` from `ensemble_pred()`
or `ensemble_hte_fit` from `ensemble_hte()`.}

\item{outcome}{Either:
\itemize{
  \item NULL (default): uses the same outcome as in ensemble fitting
  \item Character string: column name in the `data` used in ensemble fitting
  \item Numeric vector: custom outcome variable (must have same length as data,
        or same length as subset if subset is provided)
}
This allows computing BLP for a different outcome than the one used for prediction.}

\item{subset}{For `ensemble_pred_fit` with subset training (train_idx), controls 
which observations to use:
\itemize{
  \item NULL (default): uses training observations if default outcome and subset 
        training was used, otherwise uses all observations
  \item "train": uses only training observations (requires train_idx in ensemble_fit)
  \item "all": uses all observations
}
For `ensemble_hte_fit`, this can be a logical or integer vector specifying which
observations to include.}
}
\value{
An object of class `blp_pred_results` containing:
\itemize{
  \item estimates: data.table with BLP estimates averaged across repetitions
  \item outcome: outcome variable used
  \item targeted_outcome: original outcome from ensemble fitting
  \item fit_type: "hte" or "pred" depending on input
  \item n_used: number of observations used
  \item M: number of repetitions
  \item call: the function call
}
}
\description{
Computes the Best Linear Predictor (BLP) of the outcome using the ensemble
predictions (from `ensemble_pred`) or ITE predictions (from `ensemble_hte`).
This is a simple regression of Y on the predicted values.

This function implements the multiple-split estimation strategy developed in
Fava (2025), which combines predictions from multiple machine learning algorithms
into an ensemble and averages BLP estimates across M repetitions of K-fold
cross-fitting to improve statistical power.

The method uses ordinary least squares regression of Y on predicted values.
A coefficient (beta) close to 1 indicates good calibration; significantly
different from 1 suggests over- or under-prediction.

When using `ensemble_hte_fit` objects, this allows testing whether predicted
treatment effects correlate with a different outcome variable (e.g., an endline
measure that may only be observed for a subset of the data).
}
\examples{
\donttest{
set.seed(123)
n <- 500
X1 <- rnorm(n); X2 <- rnorm(n)
Y <- 2 * X1 + X2 + rnorm(n)
data <- data.frame(Y = Y, X1 = X1, X2 = X2)

fit <- ensemble_pred(Y ~ X1 + X2, data = data,
                     algorithms = c("lm", "ranger"), M = 3, K = 3)
result <- blp_pred(fit)
print(result)
}

}
\references{
Fava, B. (2025). Training and Testing with Multiple Splits: A Central Limit
Theorem for Split-Sample Estimators. \emph{arXiv preprint arXiv:2511.04957}.
}
