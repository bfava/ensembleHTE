% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ensemble_hte.R
\name{ensemble_hte}
\alias{ensemble_hte}
\title{Fit Ensemble Heterogeneous Treatment Effects Model}
\usage{
ensemble_hte(
  formula = NULL,
  treatment = NULL,
  data = NULL,
  Y = NULL,
  X = NULL,
  D = NULL,
  prop_score = NULL,
  M = 2,
  K = 3,
  algorithms = c("lm", "grf"),
  metalearner = c("r", "t", "s", "x"),
  r_learner = "grf",
  ensemble_folds = 5,
  task_type = NULL,
  scale_covariates = TRUE,
  tune = FALSE,
  tune_params = list(time = 30, cv_folds = 3, stagnation_iters = 250,
    stagnation_threshold = 0.01, measure = NULL),
  ensemble_strategy = "cv",
  n_cores = 1
)
}
\arguments{
\item{formula}{A formula specifying the outcome and covariates (e.g.,
\code{Y ~ X1 + X2} or \code{Y ~ .}). Use \code{~ . - Z} to exclude variables.
Required if \code{Y}, \code{X}, \code{D} are not provided.}

\item{treatment}{The treatment variable. Can be specified as:
\itemize{
  \item An unquoted variable name: \code{treatment = D}
  \item A quoted string: \code{treatment = "D"}
  \item A variable containing the column name: \code{treat_col <- "D"; treatment = treat_col}
  \item Ignored when using matrix interface (use \code{D} parameter instead)
}}

\item{data}{A data.frame or data.table containing the variables in the formula.
Required if using formula interface; ignored if \code{Y}, \code{X}, \code{D}
are provided.}

\item{Y}{Numeric vector of outcomes. Use this with \code{X} and \code{D} as an
alternative to the formula interface.}

\item{X}{Matrix or data.frame of covariates. Use this with \code{Y} and \code{D}
as an alternative to the formula interface.}

\item{D}{Numeric vector of treatment indicators (0/1). Use this with \code{Y}
and \code{X} as an alternative to the formula interface.}

\item{prop_score}{Numeric vector of propensity scores (probability of treatment
given covariates). If \code{NULL} (default), assumes constant propensity equal
to the sample treatment proportion (appropriate for randomized experiments).}

\item{M}{Integer. Number of sample splitting repetitions (default: 2). Higher
values improve stability but increase computation time.}

\item{K}{Integer. Number of cross-fitting folds within each repetition
(default: 3). Each observation appears in exactly one test fold per repetition.}

\item{algorithms}{Character vector of ML algorithms to include in the ensemble.
Default is \code{c("lm", "grf")}. Algorithms can come from two sources:
\itemize{
  \item \strong{grf package}: Use \code{"grf"} for generalized random forest
    (via \code{grf::regression_forest} or \code{grf::probability_forest})
  \item \strong{mlr3 learners}: Any algorithm available in mlr3 or its extensions.
    Specify just the algorithm name without the task prefix (e.g., use \code{"ranger"}
    not \code{"regr.ranger"}). The function will automatically add the appropriate
    prefix based on the task type. Common examples include:
    \itemize{
      \item \code{"lm"}: Linear regression
      \item \code{"ranger"}: Random forest
      \item \code{"glmnet"}: Elastic net regularization
      \item \code{"xgboost"}: Gradient boosting
      \item \code{"nnet"}: Neural network
      \item \code{"kknn"}: K-nearest neighbors
      \item \code{"svm"}: Support vector machine
    }
    To see all available learners, run \code{mlr3::mlr_learners$keys()}.
    Additional learners may require installing \code{mlr3learners} or
    \code{mlr3extralearners} packages.
}}

\item{metalearner}{Character. The metalearner strategy for ITE estimation.
One of:
\itemize{
  \item \code{"r"} (default): R-learner with Robinson transformation
  \item \code{"t"}: T-learner with separate models per treatment arm
  \item \code{"s"}: S-learner with treatment as a feature
  \item \code{"x"}: X-learner with imputed counterfactuals
}
See \strong{Metalearners} section below for detailed descriptions.}

\item{r_learner}{Character. When \code{metalearner = "r"}, specifies the algorithm
for estimating the conditional average treatment effect (CATE) in the final
stage. Default is \code{"grf"} (\code{grf::causal_forest}). Can be:
\itemize{
  \item \code{"grf"}: Uses \code{grf::causal_forest} (recommended)
  \item Any mlr3 learner: e.g., \code{"ranger"}, \code{"xgboost"}, \code{"glmnet"}
}
This does not need to be in the \code{algorithms} list. Only used when
\code{metalearner = "r"}.}

\item{ensemble_folds}{Integer. Number of folds for cross-validated ensemble
weight estimation (default: 5).}

\item{task_type}{Character. Type of prediction task: \code{"regr"} for
continuous outcomes or \code{"classif"} for binary outcomes. If \code{NULL}
(default), automatically detected from the outcome.}

\item{scale_covariates}{Logical. Whether to standardize non-binary numeric
covariates to mean 0 and standard deviation 1 before ML training (default:
\code{TRUE}). Binary variables (0/1) are not scaled. The original data is
preserved in the returned object.}

\item{tune}{Logical. Whether to perform hyperparameter tuning for ML algorithms
(default: \code{FALSE}). When \code{TRUE}, uses random search with early
stopping.}

\item{tune_params}{List of tuning parameters:
\itemize{
  \item \code{time}: Maximum tuning time in seconds (default: 30)
  \item \code{cv_folds}: Number of CV folds for tuning (default: 3)
  \item \code{stagnation_iters}: Stop if no improvement for this many iterations (default: 250)
  \item \code{stagnation_threshold}: Minimum improvement threshold (default: 0.01)
  \item \code{measure}: Performance measure (default: R² for regression, AUC for classification)
}}

\item{ensemble_strategy}{Character. Strategy for combining algorithm predictions.
Currently only \code{"cv"} is available, which uses cross-validated Best Linear
Predictor (BLP) regression to learn optimal weights for each algorithm.
See \strong{Ensemble Strategy} section for details.}

\item{n_cores}{Integer. Number of cores for parallel processing of repetitions.
Default is 1 (sequential). Set to higher values to parallelize the M repetitions.
Uses the \code{future} framework, so users can also set up their own parallel
backend via \code{future::plan()} before calling this function.}
}
\value{
An object of class \code{ensemble_hte_fit} containing:
\describe{
  \item{ite}{data.table of ITE predictions with M columns (one per repetition)}
  \item{call}{The matched function call}
  \item{formula}{The formula used (or constructed from Y/X/D)}
  \item{treatment}{Name of the treatment variable}
  \item{data}{The original data (or constructed data.table from Y/X/D)}
  \item{Y}{Vector of outcomes}
  \item{X}{data.table of covariates (unscaled)}
  \item{D}{Vector of treatment indicators}
  \item{prop_score}{Vector of propensity scores}
  \item{weights}{Inverse propensity weights}
  \item{splits}{List of fold assignments for each repetition}
  \item{n}{Number of observations}
  \item{M, K}{Number of repetitions and folds}
  \item{algorithms}{Algorithms used in ensemble}
  \item{metalearner}{Metalearner strategy used}
  \item{r_learner}{R-learner algorithm (if applicable)}
  \item{ensemble_folds}{Number of ensemble CV folds}
  \item{task_type}{Task type (regr or classif)}
  \item{scale_covariates}{Whether covariates were scaled}
  \item{tune, tune_params}{Tuning settings}
  \item{n_cores}{Number of cores used for parallel processing}
}
}
\description{
Estimates heterogeneous treatment effects (HTEs) using an ensemble of machine
learning algorithms combined with multiple sample splitting. This function
implements the estimation strategy developed by Fava (2025), which improves
statistical power by averaging predictions across M repetitions of K-fold
cross-fitting.

By default, the function uses the R-learner metalearner strategy with
generalized random forest (\code{grf}) as the CATE estimator.

The function supports two interfaces:
\itemize{
  \item \strong{Formula interface}: Specify \code{formula}, \code{treatment}, and \code{data}
  \item \strong{Matrix interface}: Specify \code{Y}, \code{X}, and \code{D} directly
}
}
\section{Metalearners}{

The function supports four metalearner strategies for estimating individual
treatment effects (ITEs):
\itemize{
  \item \strong{R-learner} (default): Robinson transformation with residual-on-residual
    regression. Uses \code{grf::causal_forest} by default for the final CATE model.
  \item \strong{T-learner}: Trains separate models for treated and control groups
  \item \strong{S-learner}: Trains a single model with treatment as a feature
  \item \strong{X-learner}: Two-stage approach that imputes counterfactual outcomes
}
See Nie & Wager (2021) for R-learner and Künzel et al. (2019) for T/S/X-learners.
}

\section{Ensemble Strategy}{

The ensemble combines predictions from multiple ML algorithms using a Best
Linear Predictor (BLP) approach. For each repetition, algorithm predictions
are combined via weighted least squares where weights are derived from a
cross-validated BLP regression. The \code{ensemble_strategy} parameter is
reserved for future ensemble methods (currently only "cv" is implemented).
}

\examples{
\dontrun{
# Formula interface with unquoted treatment
fit <- ensemble_hte(
  formula = outcome ~ age + income + education,
  treatment = treated,
  data = mydata,
  algorithms = c("lm", "ranger", "grf"),
  M = 5, K = 5
)

# Formula interface with quoted treatment
fit <- ensemble_hte(
  formula = outcome ~ age + income + education,
  treatment = "treated",
  data = mydata,
  algorithms = c("lm", "grf"),
  M = 5, K = 5
)

# Treatment column name stored in a variable
treat_col <- "treated"
fit <- ensemble_hte(
  formula = outcome ~ .,
  treatment = treat_col,
  data = mydata
)

# Matrix interface
fit <- ensemble_hte(
  Y = mydata$outcome,
  X = mydata[, c("age", "income", "education")],
  D = mydata$treated,
  algorithms = c("lm", "ranger"),
  M = 5, K = 5
)

# With propensity scores
fit <- ensemble_hte(
  Y ~ . - other_outcome,
  treatment = treat,
  data = mydata,
  prop_score = mydata$pscore,
  metalearner = "x"
)

# With parallel processing (4 cores)
fit <- ensemble_hte(
  Y ~ .,
  treatment = treat,
  data = mydata,
  M = 10, K = 5,
  n_cores = 4
)

# Print and summarize results
print(fit)
summary(fit)

# Use for downstream analysis
gates_results <- gates(fit)
blp_results <- blp(fit)
}

}
\references{
Fava, B. (2025). Training and Testing with Multiple Splits: A Central Limit
Theorem for Split-Sample Estimators. \emph{arXiv preprint arXiv:2511.04957}.

Nie, X., & Wager, S. (2021). Quasi-Oracle Estimation of Heterogeneous
Treatment Effects. \emph{Biometrika}, 108(2), 299-319.

Künzel, S.R., Sekhon, J.S., Bickel, P.J., & Yu, B. (2019). Metalearners for
estimating heterogeneous treatment effects using machine learning.
\emph{Proceedings of the National Academy of Sciences}, 116(10), 4156-4165.
}
\seealso{
\code{\link{gates}} for Group Average Treatment Effects analysis,
\code{\link{blp}} for Best Linear Predictor analysis,
\code{\link{clan}} for Classification Analysis,
\code{\link{ensemble_pred}} for standard prediction without treatment effects
}
