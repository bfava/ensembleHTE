% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ensemble_pred.R
\name{ensemble_pred}
\alias{ensemble_pred}
\title{Fit Ensemble Prediction Model}
\usage{
ensemble_pred(
  formula = NULL,
  data = NULL,
  Y = NULL,
  X = NULL,
  train_idx = NULL,
  M = 2,
  K = 3,
  algorithms = c("lm", "grf"),
  ensemble_folds = 5,
  task_type = NULL,
  scale_covariates = TRUE,
  tune = FALSE,
  tune_params = list(time = 30, cv_folds = 3, stagnation_iters = 250,
    stagnation_threshold = 0.01, measure = NULL),
  ensemble_strategy = c("cv", "average"),
  n_cores = 1
)
}
\arguments{
\item{formula}{A formula specifying the outcome and covariates (e.g.,
\code{Y ~ X1 + X2} or \code{Y ~ .}). Use \code{~ . - Z} to exclude variables.
Required if \code{Y} and \code{X} are not provided.}

\item{data}{A data.frame or data.table containing the variables in the formula.
Required if using formula interface; ignored if \code{Y} and \code{X}
are provided.}

\item{Y}{Numeric vector of outcomes. Use this with \code{X} as an
alternative to the formula interface. Can contain NA values for observations
where \code{train_idx = FALSE}.}

\item{X}{Matrix or data.frame of covariates. Use this with \code{Y}
as an alternative to the formula interface.}

\item{train_idx}{Optional logical or integer vector indicating which observations
to use for training. If \code{NULL} (default), all observations are used.
If provided:
\itemize{
  \item Logical vector: \code{TRUE} for training observations
  \item Integer vector: indices of training observations
}
Y must not have NA values for training observations.
See \strong{Training on a Subset} section for details on how this affects
cross-fitting and predictions.}

\item{M}{Integer. Number of sample splitting repetitions (default: 2). Higher
values improve stability but increase computation time.}

\item{K}{Integer. Number of cross-fitting folds within each repetition
(default: 3). Each observation appears in exactly one test fold per repetition.}

\item{algorithms}{Character vector of ML algorithms to include in the ensemble.
Default is \code{c("lm", "grf")}. Algorithms can come from two sources:
\itemize{
  \item \strong{grf package}: Use \code{"grf"} for generalized random forest
    (via \code{grf::regression_forest} or \code{grf::probability_forest})
  \item \strong{mlr3 learners}: Any algorithm available in mlr3 or its extensions.
    Specify just the algorithm name without the task prefix (e.g., use \code{"ranger"}
    not \code{"regr.ranger"}). The function will automatically add the appropriate
    prefix based on the task type. Common examples include:
    \itemize{
      \item \code{"lm"}: Linear regression
      \item \code{"ranger"}: Random forest
      \item \code{"glmnet"}: Elastic net regularization
      \item \code{"xgboost"}: Gradient boosting
      \item \code{"nnet"}: Neural network
      \item \code{"kknn"}: K-nearest neighbors
      \item \code{"svm"}: Support vector machine
    }
    To see all available learners, run \code{mlr3::mlr_learners$keys()}.
    Additional learners may require installing \code{mlr3learners} or
    \code{mlr3extralearners} packages.
}}

\item{ensemble_folds}{Integer. Number of folds for cross-validated ensemble
weight estimation (default: 5).}

\item{task_type}{Character. Type of prediction task: \code{"regr"} for
continuous outcomes or \code{"classif"} for binary outcomes. If \code{NULL}
(default), automatically detected from the outcome.}

\item{scale_covariates}{Logical. Whether to standardize non-binary numeric
covariates to mean 0 and standard deviation 1 before ML training (default:
\code{TRUE}). Binary variables (0/1) are not scaled. The original data is
preserved in the returned object.}

\item{tune}{Logical. Whether to perform hyperparameter tuning for ML algorithms
(default: \code{FALSE}). When \code{TRUE}, uses random search with early
stopping.}

\item{tune_params}{List of tuning parameters:
\itemize{
  \item \code{time}: Maximum tuning time in seconds (default: 30)
  \item \code{cv_folds}: Number of CV folds for tuning (default: 3)
  \item \code{stagnation_iters}: Stop if no improvement for this many iterations (default: 250)
  \item \code{stagnation_threshold}: Minimum improvement threshold (default: 0.01)
  \item \code{measure}: Performance measure (default: RÂ² for regression, AUC for classification)
}}

\item{ensemble_strategy}{Character. Strategy for combining algorithm predictions:
\itemize{
  \item \code{"cv"} (default): Cross-validated OLS regression of Y on algorithm
    predictions. Learns optimal weights for each algorithm.
  \item \code{"average"}: Simple average of all algorithm predictions. No weight
    learning; all algorithms contribute equally. Works with a single algorithm.
}
See \strong{Ensemble Strategy} section for more details.}

\item{n_cores}{Integer. Number of cores for parallel processing of repetitions.
Default is 1 (sequential). Set to higher values to parallelize the M repetitions.
Uses the \code{future} framework, so users can also set up their own parallel
backend via \code{future::plan()} before calling this function.}
}
\value{
An object of class \code{ensemble_pred_fit} containing:
\describe{
  \item{predictions}{data.table of predictions with M columns (one per repetition)}
  \item{call}{The matched function call}
  \item{formula}{The formula used (or constructed from Y/X)}
  \item{data}{The original data (or constructed data.table from Y/X)}
  \item{Y}{Vector of outcomes}
  \item{X}{data.table of covariates (unscaled)}
  \item{train_idx}{Logical vector indicating training observations}
  \item{splits}{List of fold assignments for each repetition}
  \item{n}{Number of observations}
  \item{n_train}{Number of training observations}
  \item{M, K}{Number of repetitions and folds}
  \item{algorithms}{Algorithms used in ensemble}
  \item{ensemble_folds}{Number of ensemble CV folds}
  \item{ensemble_strategy}{Strategy used for combining predictions ("cv" or "average")}
  \item{task_type}{Task type (regr or classif)}
  \item{scale_covariates}{Whether covariates were scaled}
  \item{tune, tune_params}{Tuning settings}
  \item{n_cores}{Number of cores used for parallel processing}
}
}
\description{
Predicts an outcome variable Y using an ensemble of machine learning
algorithms combined with multiple sample splitting. This function
implements the estimation strategy developed by Fava (2025), which improves
statistical power by averaging predictions across M repetitions of K-fold
cross-fitting.

Unlike \code{\link{ensemble_hte}} which estimates heterogeneous treatment effects,
this function performs standard prediction of Y given X without any treatment
variable or causal structure.

The function supports two interfaces:
\itemize{
  \item \strong{Formula interface}: Specify \code{formula} and \code{data}
  \item \strong{Matrix interface}: Specify \code{Y} and \code{X} directly
}
}
\section{Training on a Subset}{

The \code{train_idx} parameter allows training models on a subset of observations
while generating predictions for all observations. This is useful when the outcome
Y is only observed for some units (e.g., only treated units in an experiment) but
you want predictions for everyone.

When \code{train_idx} is provided:
\itemize{
  \item Y can have NA values for observations where \code{train_idx = FALSE}
  \item Cross-fitting splits ALL observations into K folds, stratifying by \code{train_idx}
  \item For each fold k, models are trained on training observations NOT in fold k
  \item Predictions are generated for ALL observations in fold k (both training and non-training)
  \item Each observation gets exactly one prediction per repetition (from the model where they were in the test fold)
  \item Ensemble weights are estimated using only training observations
  \item Summary statistics are computed only on training observations
}
}

\section{Ensemble Strategy}{

The ensemble combines predictions from multiple ML algorithms. The
\code{ensemble_strategy} parameter controls how predictions are combined:
\itemize{
  \item \code{"cv"} (default): Uses cross-validated OLS regression of Y on the
    predicted values from each algorithm to learn optimal combination weights.
  \item \code{"average"}: Uses simple averaging across all algorithm predictions
    with equal weights. This is useful when you want a simpler ensemble or when
    using only a single algorithm.
}
}

\examples{
\dontrun{
# Formula interface
fit <- ensemble_pred(
  formula = outcome ~ age + income + education,
  data = mydata,
  algorithms = c("lm", "ranger", "grf"),
  M = 5, K = 5
)

# Matrix interface
fit <- ensemble_pred(
  Y = mydata$outcome,
  X = mydata[, c("age", "income", "education")],
  algorithms = c("lm", "ranger"),
  M = 5, K = 5
)

# Training on a subset (e.g., outcome only observed for treated units)
# Y has NA for control units
fit <- ensemble_pred(
  formula = outcome ~ age + income,
  data = mydata,
  train_idx = mydata$treated == 1,  # Only train on treated
  M = 5, K = 5
)
# Predictions are available for ALL observations
# Use clan() or gavs() to analyze predictions

# With parallel processing (4 cores)
fit <- ensemble_pred(
  Y ~ .,
  data = mydata,
  M = 10, K = 5,
  n_cores = 4
)

# Print and summarize results
print(fit)
summary(fit)

# Use for downstream analysis
gavs_results <- gavs(fit)
blp_results <- blp_pred(fit)
}

}
\references{
Fava, B. (2025). Training and Testing with Multiple Splits: A Central Limit
Theorem for Split-Sample Estimators. \emph{arXiv preprint arXiv:2511.04957}.
}
\seealso{
\code{\link{gavs}} for Group Averages analysis,
\code{\link{blp_pred}} for Best Linear Predictor analysis,
\code{\link{clan}} for Classification Analysis,
\code{\link{ensemble_hte}} for heterogeneous treatment effect estimation
}
