% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/analysis_hte.R
\name{gavs}
\alias{gavs}
\title{Compute GAVS (Group Averages)}
\usage{
gavs(ensemble_fit, n_groups = 3, outcome = NULL, subset = NULL, strata = NULL)
}
\arguments{
\item{ensemble_fit}{An object of class \code{ensemble_hte_fit} from 
\code{ensemble_hte()} or \code{ensemble_pred_fit} from \code{ensemble_pred()}.}

\item{n_groups}{Number of groups to divide the sample into (default: 3)}

\item{outcome}{Either:
\itemize{
  \item NULL (default): uses the same outcome as in the ensemble function
  \item Character string: column name in the \code{data} used in the ensemble function
  \item Numeric vector: custom outcome variable (must have appropriate length)
}
This allows computing GAVS for a different outcome than the one used for prediction.}

\item{subset}{Controls which observations to use for evaluation:
\itemize{
  \item \code{NULL} (default): For \code{ensemble_pred_fit} with \code{train_idx},
    uses training observations when \code{outcome = NULL}; otherwise uses all.
    For \code{ensemble_hte_fit}, uses all observations.
  \item \code{"train"}: Use only training observations (only valid for
    \code{ensemble_pred_fit} with \code{train_idx}).
  \item \code{"all"}: Use all observations.
  \item Logical vector: TRUE/FALSE for each observation (length must equal
    number of rows in data).
  \item Integer vector: Indices of observations to use.
}
See \strong{Subsample Usage} section for guidance on when to use each option.}

\item{strata}{Optional. Stratification variable for restricted ranking:
\itemize{
  \item NULL (default): unrestricted ranking across full sample within folds
  \item Character string: column name in the \code{data} for stratified ranking
  \item Numeric/factor vector: strata indicator (must have same length as data)
}
When specified, predicted values are ranked within each stratum (and fold),
rather than across the full sample.}
}
\value{
An object of class \code{gavs_results} containing:
\itemize{
  \item estimates: data.table with GAVS estimates averaged across repetitions
  \item top_bottom: data.table with the top-bottom difference test
  \item all: data.table with the overall mean (weighted avg of all groups)
  \item top_all: data.table with the top minus average test
  \item n_groups: number of groups used
  \item outcome: the outcome variable used for GAVS
  \item targeted_outcome: the outcome used for prediction
  \item fit_type: "hte" or "pred" depending on input
  \item strata: the stratification variable used (if any)
  \item n_used: number of observations used
  \item M: number of repetitions
  \item call: the function call
}
}
\description{
Computes Group Averages (GAVS), which measures average outcomes for groups
defined by predicted value quantiles. This works for both heterogeneous 
treatment effect estimation (groups by predicted ITE) and prediction problems
(groups by predicted Y).

This function implements the multiple-split estimation strategy developed in
Fava (2025), which combines predictions from multiple machine learning algorithms
into an ensemble and averages GAVS estimates across M repetitions of K-fold
cross-fitting to improve statistical power.

The method uses ordinary least squares regression of Y on group indicators.
Groups are formed by ranking predicted values within each fold.
Robust (HC1) standard errors are computed.
}
\section{Subsample Usage}{

The \code{subset} parameter controls which observations are used for evaluation.
This is useful when:
\itemize{
  \item The ML model was trained on a subset (e.g., using \code{train_idx} in
    \code{ensemble_pred()}) and you want to evaluate on the same or different subset.
  \item You want to evaluate treatment effect targeting on an outcome that is
    only observed for a subset of observations.
}

A message is printed when either the ML model was trained on a subset or
the evaluation uses a subset, to help avoid unintended subsetting.
}

\examples{
\donttest{
set.seed(123)
n <- 500
X1 <- rnorm(n); X2 <- rnorm(n)
D <- rbinom(n, 1, 0.5)
Y <- X1 + D * (1 + 0.5 * X1) + rnorm(n)
data <- data.frame(Y = Y, D = D, X1 = X1, X2 = X2)

fit <- ensemble_hte(Y ~ X1 + X2, treatment = D, data = data,
                    algorithms = c("lm", "grf"), M = 3, K = 3)
result <- gavs(fit, n_groups = 3)
print(result)
plot(result)
}

}
\references{
Fava, B. (2025). Training and Testing with Multiple Splits: A Central Limit
Theorem for Split-Sample Estimators. \emph{arXiv preprint arXiv:2511.04957}.
}
